{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP(nltk)",
      "provenance": [],
      "authorship_tag": "ABX9TyNk/855lQZacWIxF1TnnMsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditi1511/python-beginner-projects/blob/master/NLP(nltk).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwcllTqrfbKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "def do_downloads():\n",
        "  nltk.download('punkt')               # sentence tokenizer\n",
        "  nltk.download('averaged_perceptron_tagger')  # pos tagger\n",
        "  nltk.download('maxent_ne_chunker')           # NE tagger\n",
        "  nltk.download('words')\n",
        "  nltk.download('stopwords') \n",
        "  nltk.download('vader_lexicon')\n",
        "  nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKIaULE1iNKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_tokenize_demo(text):\n",
        "  for sentence in nltk.sent_tokenize(text):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    print(tokens)\n",
        "    \n",
        "demo = \"This is a simple sentence. Followed by another!\"\n",
        "#nltk_tokenize_demo(demo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MAJ5VapjPME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_pos_demo(text):\n",
        "  for sent in nltk.sent_tokenize(text):\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    for t in tagged:\n",
        "      print(t)\n",
        "\n",
        "demo = \"This is a simple sentence. Followed by another!\"\n",
        "#nltk_pos_demo(demo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4u4iXGTjSZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_ne_demo(text):\n",
        "  for sent in nltk.sent_tokenize(text):\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    for chunk in nltk.ne_chunk(tagged):\n",
        "      print(chunk)\n",
        "\n",
        "demo = 'San Francisco considers banning sidewalk delivery robots'\n",
        "#nltk_ne_demo(demo)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o63p2G09kMsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 = 'San Francisco considers banning sidewalk delivery robots'\n",
        "s2 = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'\n",
        "#nltk_ne_demo(s2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvcrlnAnkSdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_find_people_demo(text):\n",
        "  for sent in nltk.sent_tokenize(text):\n",
        "    tagged = nltk.pos_tag(nltk.word_tokenize(sent))\n",
        "    for chunk in nltk.ne_chunk(tagged):\n",
        "      if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "        name = ' '.join(c[0] for c in chunk)\n",
        "        print(name)\n",
        "\n",
        "s3 = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'\n",
        "#nltk_find_people_demo(s3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1NbYvHR3PeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def nltk_stop_word_demo():\n",
        "  stop_words = stopwords.words('english')\n",
        "  print(\"nltk\", stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGQr020i3Qvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "from nltk import ngrams\n",
        "\n",
        "def nltk_ngram_demo(text):\n",
        "  tokens = text.lower().split()\n",
        "  grams = ngrams(tokens, 2)\n",
        "  \n",
        "  c = collections.Counter(grams)\n",
        "  print(c.most_common(10))\n",
        "\n",
        "text = \"We went to a clump of bushes, and Tom made everybody swear to keep the secret, and then showed them a hole in the hill, right in the thickest part of the bushes. \"\n",
        "#nltk_ngram_demo(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMwGiWr93UOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def nltk_sentiment_demo():\n",
        "  sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "  # helper\n",
        "  def polarity_scores(doc):\n",
        "      return sentiment_analyzer.polarity_scores(doc)\n",
        "  doc1 = \"INFO 490 is so fun.\"\n",
        "  doc2 = \"INFO 490 is so awful.\"\n",
        "  doc3 = \"INFO 490 is so fun that I can't wait to take the follow on course!\"\n",
        "  doc4 = \"INFO 490 is so awful that I am glad there's not a follow on course!\"\n",
        "  print(polarity_scores(doc1)) # most positive\n",
        "  print(polarity_scores(doc2)) # most negative\n",
        "  print(polarity_scores(doc3)) # mostly positive, neutral\n",
        "  print(polarity_scores(doc4)) # mostly negative, a little positive too\n",
        "\n",
        "#nltk_sentiment_demo()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaSr-5Yy3Zzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_stem_and_lemm_demo():\n",
        "\n",
        "  words = [\"game\",\"gaming\",\"gamed\",\"games\",\"gamer\",\"grows\",\"fairly\",\"nonsensical\"]\n",
        "\n",
        "  ps  = nltk.stem.PorterStemmer()\n",
        "  sno = nltk.stem.SnowballStemmer('english')\n",
        "  lan = nltk.stem.lancaster.LancasterStemmer()\n",
        " \n",
        "  for word in words:\n",
        "    base  = ps.stem(word)\n",
        "    sbase = sno.stem(word)\n",
        "    lbase = lan.stem(word)\n",
        "  \n",
        "    s = ''\n",
        "    if (sbase != base):\n",
        "      s += \"(or {})\".format(sbase)\n",
        "    if (lbase != base and lbase != sbase):\n",
        "      s += \"(or {})\".format(lbase)\n",
        "  \n",
        "    print(\"{:11s} stems to {:s} {}\".format(word, base, s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3T8Wkax3dKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('wordnet')\n",
        "\n",
        "def nltk_wordnet_demo():\n",
        "  lemma = nltk.stem.WordNetLemmatizer()\n",
        "  print(lemma.lemmatize('dogs'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAPfk99AmuTk",
        "colab_type": "code",
        "outputId": "c35dd4a2-ad44-41a6-c9fb-5f2d30c22b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import collections\n",
        "#import nltk\n",
        "HUCK_URL= \"https://raw.githubusercontent.com/NSF-EC/INFO490Assets/master/src/datasets/pg/huckfinn/huck.txt\"\n",
        "\n",
        "def read_remote(url):\n",
        "    import requests\n",
        "    with requests.get(url) as response:\n",
        "      response.encoding = 'utf-8'\n",
        "      return response.text\n",
        "\n",
        "def find_characters_nlp(text, topn):\n",
        "  # tokens = nltk.word_tokenize(text)\n",
        "  # counted = collections.Counter(tokens)\n",
        "  names = []\n",
        "  for sent in nltk.sent_tokenize(text):\n",
        "    tagged = nltk.pos_tag(nltk.word_tokenize(sent))\n",
        "    for chunk in nltk.ne_chunk(tagged):\n",
        "      if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "        name = ' '.join(c[0] for c in chunk)\n",
        "        names.append(name)\n",
        "  counted = collections.Counter(names)\n",
        "  return counted.most_common(topn)\n",
        "  # return names\n",
        "  # returns top n characters found in text\n",
        "\n",
        "text = read_remote(HUCK_URL)\n",
        "topn = find_characters_nlp(text, 50)\n",
        "print(topn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Jim', 336), ('Tom', 150), ('Huck', 41), ('Tom Sawyer', 37), ('Aunt Sally', 37), ('Buck', 32), ('Mary Jane', 23), ('Sid', 15), ('William', 15), ('Miss Watson', 14), ('Huck Finn', 14), ('Mars Tom', 14), ('Bill', 13), ('Harvey', 13), ('Miss Mary Jane', 13), ('Uncle Silas', 13), ('Peter', 12), ('George', 11), ('Mary', 10), ('Miss', 10), ('Bob', 10), ('Bilgewater', 10), ('Pap', 8), ('Ben Rogers', 7), ('Dey', 7), ('Cairo', 7), ('Susan', 7), ('Peter Wilks', 7), ('Aunt', 6), ('Jackson', 6), ('Le', 6), ('Dat', 6), ('Miss Sophia', 6), ('Boggs', 6), ('Miss Mary', 6), ('Phelps', 6), ('Sally', 6), ('Aunty', 6), ('Hello', 5), ('Doan', 5), ('Sarah', 5), ('Packard', 5), ('Levi Bell', 5), ('Mr. Lothrop', 5), ('Silas', 5), ('Aunt Polly', 4), ('Watson', 4), ('Jo Harper', 4), ('Old', 4), ('Goshen', 4)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}